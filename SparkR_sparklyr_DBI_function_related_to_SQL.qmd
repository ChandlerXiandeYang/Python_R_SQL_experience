#Stage3
# -------------------------------------------------------------------------
# Global Setup: Define dependencies and objects needed by the entire app
# -------------------------------------------------------------------------
# Load all required packages
#reactiveConsole(TRUE)
reactlog::reactlog_enable()

# This is a better way to ensure packages are installed and at the correct version
packages <- list(
AER = "1.2-13",    boot = "1.3-31",       chromote = "0.4.0", 
DBI = "1.2.3",     DiagrammeR = "1.0.11", DiagrammeRsvg = "0.1", 
dplyr = "1.1.4",   dunn.test = "1.3.6",   easyanova = "1.1.0", 
gt = "0.11.0",     forcats = "1.0.0",     formattable = "0.2.1", 
qcc = "2.7",       gridExtra = "2.3",     kableExtra = "1.4.0", 
knitr = "1.48",    lubridate = "1.9.3",   ggplot2 = "3.5.1",
lmtest = "0.9-40", openxlsx = "4.2.7.1",  odbc = "1.5.0", 
lme4 = "1.1-35.5", quarto = "1.4.4",      reactlog="1.1.1", 
rlang = "1.1.4",   rmarkdown = "2.28",    readxl = "1.3.1", 
rsvg = "2.6.1",    shiny = "1.9.1",       stringr = "1.5.1", 
tidyr = "1.3.1",   tinytex = "0.53",      VCA = "1.5.1",
webshot2 = "0.1.1")

if (!requireNamespace("devtools", quietly = TRUE))
  { install.packages("devtools") 
  }
library(devtools)

install_and_load <- function(package, version) {
  if (!requireNamespace(package, quietly = TRUE) || packageVersion(package) < version) {
    install_version(package, version = as.character(version))
  }
  library(package, character.only = TRUE)
}
for (pkg in names(packages)) { install_and_load(pkg, packages[[pkg]]) }
library(CleaningValidation)

# The following objects are expected from the parent script (Launcher.R).
# For the script to be runnable, they would need to be defined here.
# For example:
 Your_email_name <-"xyang1"
 install_folder <- paste0("C:\\Users\\", Your_email_name)
 conn1 <- DBI::dbConnect(
   drv = odbc::databricks(),
   httpPath = "/sql/1.0/warehouses/b2ef4b7a2c18fdcb",
   workspace = "adb-4117502575300068.8.azuredatabricks.net",
   authMech = 11, auth_flow = 2
 )
 data_folder<-"infolabs_dev"

# This function generates and saves a list of equipment, which is then loaded.
# It MUST run before the UI is defined, so the 'Sites' variable is available.
generate_equipment_data <- function() {
  LPDE_Weston_Stage3_Equipment <- dbGetQuery(conn1, paste0("SELECT * FROM ", data_folder, ".cv_processed.LPDE_Weston_Stage3_Equipment"))[, 1]
  LPDE_Signet_Stage3_Equipment <- dbGetQuery(conn1, paste0("SELECT * FROM ", data_folder, ".cv_processed.LPDE_Signet_Stage3_Equipment"))[, 1]
  LPDE_Garyray_Stage3_Equipment <- dbGetQuery(conn1, paste0("SELECT * FROM ", data_folder, ".cv_processed.LPDE_Garyray_Stage3_Equipment"))[, 1]
  LPDE_Etobicoke_Stage3_Equipment <- dbGetQuery(conn1, paste0("SELECT * FROM ", data_folder, ".cv_processed.LPDE_Etobicoke_Stage3_Equipment"))[, 1]
  
LPDE_Signet_Etobicoke <- c(LPDE_Weston_Stage3_Equipment, LPDE_Signet_Stage3_Equipment, LPDE_Garyray_Stage3_Equipment, LPDE_Etobicoke_Stage3_Equipment)
Sites <- c("LPDE_Weston_Stage3_Equipment", "LPDE_Signet_Stage3_Equipment", "LPDE_Garyray_Stage3_Equipment", "LPDE_Etobicoke_Stage3_Equipment")
  
save(LPDE_Weston_Stage3_Equipment, LPDE_Signet_Stage3_Equipment, LPDE_Garyray_Stage3_Equipment, LPDE_Etobicoke_Stage3_Equipment,
       LPDE_Signet_Etobicoke, Sites,
       file = paste0(install_folder, "\\CV_SARWAS_LPDE\\LowPDE_Templates\\_CV_Low_PDE_List_of_Equipment3.RData"))
}
generate_equipment_data()
load(paste0(install_folder, "\\CV_SARWAS_LPDE\\LowPDE_Templates\\_CV_Low_PDE_List_of_Equipment3.RData"))

# -------------------------------------------------------------------------
# UI: User interface definition
# -------------------------------------------------------------------------
ui <- fluidPage(
  tags$head(
    tags$style(HTML("
    .titlePanel { background-color: #337ab7; color: white; font-weight: bold; text-align: center; }
    #generate_report, #download_report, #download_data, #generate_data {
      color: white; background: linear-gradient(#3E8CCF, #337ab7); font-weight: bold;
      border: 1px solid #2c6ca3; border-radius: 5px; padding: 10px; cursor: pointer;
      box-shadow: 3px 3px 5px rgba(0, 0, 0, 0.4), 6px 6px 10px rgba(0, 0, 0, 0.2);
      transition: background-color 0.3s, box-shadow 0.3s; width: 100%; margin-bottom: 15px;
    }
    #generate_report:hover, #download_report:hover, #download_data:hover, #generate_data:hover {
      background-color: #2c6ca3; box-shadow: 2px 2px 4px #2c6ca3;
    }
    .progress-text { color: #3E8CCF; font-weight: bold; text-align: center; margin-bottom: 15px; }
    .button-container {
      display: flex; flex-direction: column; justify-content: space-between; align-items: stretch; gap: 15px;
    }
  "))
  ) ,
  
  div(class = "titlePanel", titlePanel("CV_SARWAS_Low_PDE_Stage3")),
  
  sidebarLayout(
    sidebarPanel(
      selectInput("site", "Select Site", choices = Sites),
      uiOutput("equipment_dropdown"),
      dateInput("Time_Period_Initial", "Start Date", value = Sys.Date() - 10000),
      dateInput("Time_Period_End", "End Date", value = Sys.Date()),
      actionButton("generate_report", HTML("<b>Generate Equipment Report</b>")),
      verbatimTextOutput("progress_report"),
      downloadButton("download_report", "Download Equipment Report"),
      actionButton("generate_data", HTML("<b>Generate Equipment Excel Data</b>")),
      verbatimTextOutput("progress_data"),
      downloadButton("download_data", "Download Equipment Excel Data")
    ),
    
    mainPanel(
      HTML("<p><b>To Generate And Download Annual Statistical Report:</b><br>
            1. Select a site.<br>
            2. Choose a piece of equipment.<br>
            3. Select the time period.<br>
            4. Click 'Generate Equipment Report' and wait for 'Progress: 100% Done'.<br>
            5. Click 'Download Equipment Report' to get the report.<br><br>
            <b>To Generate And Download Equipment Excel Data:</b><br>
            1. Select a site.<br>
            2. Choose a piece of equipment.<br>
            3. Select the time period.<br>
            4. Click 'Generate Equipment Excel Data' and wait for 'Progress: 100% Done'.<br>
            5. Click 'Download Equipment Excel Data' to download.<br><br>"),
      
      textAreaInput("maintenance_contact", "Contact Information", value = "xyang1@apotex.com, Tel:416-8888-888")
    )
  )
)

# -------------------------------------------------------------------------
# Server: Backend logic
# -------------------------------------------------------------------------
server <- function(input, output, session) {
  
  output$equipment_dropdown <- renderUI({
    site_name <- input$site
    equipment_list <- switch(site_name,
                             "LPDE_Weston_Stage3_Equipment"=LPDE_Weston_Stage3_Equipment,
                             "LPDE_Signet_Stage3_Equipment"=LPDE_Signet_Stage3_Equipment,
                             "LPDE_Garyray_Stage3_Equipment"=LPDE_Garyray_Stage3_Equipment,
                             "LPDE_Etobicoke_Stage3_Equipment"=LPDE_Etobicoke_Stage3_Equipment
    )
    selectInput("equipment", "Select Equipment", choices = equipment_list, multiple = FALSE)
  })
  
  Report_Writing_Identifier <- reactiveVal(NULL)
  
  observe({
    req(input$equipment)
    Report_Writing_Identifier(input$equipment)
  })
  
  observeEvent(input$generate_report, {
    equipment_names <- input$equipment
    Time_Period_Initial <- as.character(input$Time_Period_Initial)
    Time_Period_End <- as.character(input$Time_Period_End)
    original_working_directory <- paste0(install_folder, "\\CV_SARWAS_LPDE\\LowPDE_Templates")
    withProgress(
      session = session,
      message = 'Generating Reports...',
      detail = '',
      value = 0,
      {for (i in seq_along(equipment_names)) {
          Report_Writing_Identifier <- equipment_names[i]
          Sys.setenv(REPORT_WRITING_IDENTIFIER = Report_Writing_Identifier)
          Sys.setenv(TIME_PERIOD_INITIAL = Time_Period_Initial)
          Sys.setenv(TIME_PERIOD_END = Time_Period_End)
          Sys.setenv(INSTALL_FOLDER=install_folder )
          output_file <- paste0("0", equipment_names[i], ".pdf")
          setwd(original_working_directory)
          quarto::quarto_render(
            file.path(original_working_directory, "_cv08_1_Low_PDE_Automation_Stage3.qmd"),
            output_file = output_file
          )
          setwd(original_working_directory)
          new_output_file <- file.path(paste0(install_folder, "\\cv_sarwas_lpde\\Reports_and_Data_Stage3_LPDE\\", equipment_names[i], ".pdf"))
          file.rename(output_file, new_output_file)
          
          incProgress(1 / length(equipment_names), detail = paste('Generated report for', equipment_names[i]))
        }
        
        setProgress(1, message = 'Done!')
      } ) })
  
  output$download_report <- downloadHandler(
    filename = function() {
      paste("0", input$equipment, ".pdf", sep = "")
    },
    content = function(file) {
      pdf_files <- file.path(paste0(install_folder,"\\CV_SARWAS_LPDE\\Reports_and_Data_Stage3_LPDE\\", input$equipment, ".pdf"))
      pdftools::pdf_combine(pdf_files, output = file)
    }
  )
  
  output$progress_report <- renderText({
    if (!isTruthy(input$generate_report)) {
      return("Progress: 0% Done")
    }
    "Progress: 100% Done"
  })
  
  current_queries <- reactiveVal(NULL)
  queries <- reactive({
    equipment_name <- as.character(Report_Writing_Identifier())
    list(
      DAR = sprintf("SELECT * FROM infolabs_dev.cv_processed.LPDE_Stage3_signet_garyray_weston_etobicoke WHERE DataSetName='%s'", equipment_name)
      
    )
  })
  
  observeEvent(input$generate_data, {
    Time_Period_Initial <- as.Date(input$Time_Period_Initial, format = "%Y-%m-%d")
    Time_Period_End <- as.Date(input$Time_Period_End, format = "%Y-%m-%d")
    
    withProgress(
      session = session,
      message = 'Generating Data...',
      detail = '',
      value = 0,
      {
        Report_Writing_Identifier <- input$equipment[1]
        Sys.setenv(REPORT_WRITING_IDENTIFIER = Report_Writing_Identifier)
        current_queries_val <- queries()
        total_queries <- length(current_queries_val)
        wb <- openxlsx::createWorkbook()
        for (i in seq_along(current_queries_val)) {
          sheet_name <- names(current_queries_val)[i]
          openxlsx::addWorksheet(wb, sheet_name)
          data_frame <- tryCatch({
            dbGetQuery(conn1, current_queries_val[[i]])
          }, error = function(e) {
            return(data.frame())
          })
          if (!inherits(data_frame$Sampling_Date, "POSIXct")) {
            data_frame$Sampling_Date <- as.POSIXct(data_frame$Sampling_Date)
          }
          
          # Corrected logical error to include the entire end date
          filtered_data <- data_frame[data_frame$Sampling_Date >= Time_Period_Initial &
                                        data_frame$Sampling_Date < (Time_Period_End+1), ]
          
          openxlsx::writeData(wb, sheet = sheet_name, x = filtered_data)
          incProgress(1 / total_queries, detail = sprintf('%d/%d queries completed', i, total_queries))
        }
        xlsx_file <- file.path(paste0(install_folder, "\\CV_SARWAS_LPDE\\Reports_and_Data_Stage3_LPDE\\", input$equipment[1], ".xlsx"))
        #xlsx_file <- file.path("C:\\Users\\xyang1\\CV_SARWAS_LPDE\\Reports_and_Data_Stage3_LPDE", paste0(input$equipment[1], ".xlsx"))
        openxlsx::saveWorkbook(wb, xlsx_file, overwrite = TRUE)
        incProgress(1, detail = 'Done!')
      }
    )
  })
  
  output$download_data <- downloadHandler(
    filename = function() {
      paste0(input$equipment[1], ".xlsx")
    },
    content = function(file) {
      xlsx_file <- file.path(paste0(install_folder, "\\CV_SARWAS_LPDE\\Reports_and_Data_Stage3_LPDE\\", input$equipment[1], ".xlsx"))
      #xlsx_file <- file.path(paste0(install_folder, "\\CV_SARWAS_LPDE\\Reports_and_Data_Stage3_LPDE"), paste0(input$equipment[1], ".xlsx"))
      file.copy(xlsx_file, file)
    }
  )
  
  observe({
    current_queries_val <- current_queries()
    
    if (input$generate_data > 0 && !is.null(current_queries_val)) {
      withProgress(
        session = session,
        message = 'Generating Data for Tabs...',
        detail = '',
        value = 0,
        {
          for (i in seq_along(current_queries_val)) {
            incProgress(1/length(current_queries_val), detail = sprintf('%d/%d tabs completed', i, length(current_queries_val)))
            Sys.sleep(0.1)
          }
          incProgress(1, message = 'Data Generation Complete!')
        })}})
  
  output$progress_data <- renderText({
    if (!isTruthy(input$generate_data)) {
      return("Progress: 0% Done")
    }
    "Progress: 100% Done"
  })}
runApp(list(ui = ui, server = server), launch.browser = TRUE)
shiny::reactlogShow()


Stage 2

# -------------------------------------------------------------------------
# Global Setup: Define dependencies and objects needed by the entire app
# -------------------------------------------------------------------------
# Load all required packages
reactiveConsole(TRUE)
reactlog::reactlog_enable()

# This is a better way to ensure packages are installed and at the correct version
packages <- list(
  AER = "1.2-13",    boot = "1.3-31",       chromote = "0.4.0", 
  DBI = "1.2.3",     DiagrammeR = "1.0.11", DiagrammeRsvg = "0.1", 
  dplyr = "1.1.4",   dunn.test = "1.3.6",   easyanova = "1.1.0", 
  gt = "0.11.0",     forcats = "1.0.0",     formattable = "0.2.1", 
  qcc = "2.7",       gridExtra = "2.3",     kableExtra = "1.4.0", 
  knitr = "1.48",    lubridate = "1.9.3",   ggplot2 = "3.5.1",
  lmtest = "0.9-40", openxlsx = "4.2.7.1",  odbc = "1.5.0", 
  lme4 = "1.1-35.5", quarto = "1.4.4",      reactlog="1.1.1", 
  rlang = "1.1.4",   rmarkdown = "2.28",    readxl = "1.3.1", 
  rsvg = "2.6.1",    shiny = "1.9.1",       stringr = "1.5.1", 
  tidyr = "1.3.1",   tinytex = "0.53",      VCA = "1.5.1",
  webshot2 = "0.1.1")

if (!requireNamespace("devtools", quietly = TRUE))
{ install.packages("devtools") 
}
library(devtools)

install_and_load <- function(package, version) {
  if (!requireNamespace(package, quietly = TRUE) || packageVersion(package) < version) {
    install_version(package, version = as.character(version))
  }
  library(package, character.only = TRUE)
}
for (pkg in names(packages)) { install_and_load(pkg, packages[[pkg]]) }
library(CleaningValidation)

# The following objects are expected from the parent script (Launcher.R).
# For the script to be runnable, they would need to be defined here.
# For example:
# Your_email_name <-"xyang1"
# install_folder <- paste0("C:\\Users\\", Your_email_name)
# conn1 <- DBI::dbConnect(
#   drv = odbc::databricks(),
#   httpPath = "/sql/1.0/warehouses/b2ef4b7a2c18fdcb",
#   workspace = "adb-4117502575300068.8.azuredatabricks.net",
#   authMech = 11, auth_flow = 2
# )

# This function generates and saves a list of equipment, which is then loaded.
# It MUST run before the UI is defined, so the 'Sites' variable is available.
generate_equipment_data <- function() {
  LPDE_Weston_Stage2_Equipment <- dbGetQuery(conn1, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Weston_Stage2_Equipment")[, 1]
  LPDE_Signet_Stage2_Equipment <- dbGetQuery(conn1, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Signet_Stage2_Equipment")[, 1]
  LPDE_Garyray_Stage2_Equipment <- dbGetQuery(conn1, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Garyray_Stage2_Equipment")[, 1]
  LPDE_Etobicoke_Stage2_Equipment <- dbGetQuery(conn1, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Etobicoke_Stage2_Equipment")[, 1]
  
  LPDE_Signet_Etobicoke <- c(LPDE_Weston_Stage2_Equipment, LPDE_Signet_Stage2_Equipment, LPDE_Garyray_Stage2_Equipment, LPDE_Etobicoke_Stage2_Equipment)
  Sites <- c("LPDE_Weston_Stage2_Equipment", "LPDE_Signet_Stage2_Equipment", "LPDE_Garyray_Stage2_Equipment", "LPDE_Etobicoke_Stage2_Equipment")
  
  save(LPDE_Weston_Stage2_Equipment, LPDE_Signet_Stage2_Equipment, LPDE_Garyray_Stage2_Equipment, LPDE_Etobicoke_Stage2_Equipment,
       LPDE_Signet_Etobicoke, Sites,
       file = paste0(install_folder, "\\CV_SARWAS_LPDE\\LowPDE_Templates\\_CV_Low_PDE_List_of_Equipment2.RData"))
}
generate_equipment_data()
load(paste0(install_folder, "\\CV_SARWAS_LPDE\\LowPDE_Templates\\_CV_Low_PDE_List_of_Equipment2.RData"))

# -------------------------------------------------------------------------
# UI: User interface definition
# -------------------------------------------------------------------------
ui <- fluidPage(
  tags$head(
    tags$style(HTML("
    .titlePanel { background-color: #337ab7; color: white; font-weight: bold; text-align: center; }
    #generate_report, #download_report, #download_data, #generate_data {
      color: white; background: linear-gradient(#3E8CCF, #337ab7); font-weight: bold;
      border: 1px solid #2c6ca3; border-radius: 5px; padding: 10px; cursor: pointer;
      box-shadow: 3px 3px 5px rgba(0, 0, 0, 0.4), 6px 6px 10px rgba(0, 0, 0, 0.2);
      transition: background-color 0.3s, box-shadow 0.3s; width: 100%; margin-bottom: 15px;
    }
    #generate_report:hover, #download_report:hover, #download_data:hover, #generate_data:hover {
      background-color: #2c6ca3; box-shadow: 2px 2px 4px #2c6ca3;
    }
    .progress-text { color: #3E8CCF; font-weight: bold; text-align: center; margin-bottom: 15px; }
    .button-container {
      display: flex; flex-direction: column; justify-content: space-between; align-items: stretch; gap: 15px;
    }
  "))
  ) ,
  
  div(class = "titlePanel", titlePanel("CV_SARWAS_Low_PDE_Stage2")),
  
  sidebarLayout(
    sidebarPanel(
      selectInput("site", "Select Site", choices = Sites),
      uiOutput("equipment_dropdown"),
      dateInput("Time_Period_Initial", "Start Date", value = Sys.Date() - 10000),
      dateInput("Time_Period_End", "End Date", value = Sys.Date()),
      actionButton("generate_report", HTML("<b>Generate Equipment Report</b>")),
      verbatimTextOutput("progress_report"),
      downloadButton("download_report", "Download Equipment Report"),
      actionButton("generate_data", HTML("<b>Generate Equipment Excel Data</b>")),
      verbatimTextOutput("progress_data"),
      downloadButton("download_data", "Download Equipment Excel Data")
    ),
    
    mainPanel(
      HTML("<p><b>To Generate And Download Annual Statistical Report:</b><br>
            1. Select a site.<br>
            2. Choose a piece of equipment.<br>
            3. Select the time period.<br>
            4. Click 'Generate Equipment Report' and wait for 'Progress: 100% Done'.<br>
            5. Click 'Download Equipment Report' to get the report.<br><br>
            <b>To Generate And Download Equipment Excel Data:</b><br>
            1. Select a site.<br>
            2. Choose a piece of equipment.<br>
            3. Select the time period.<br>
            4. Click 'Generate Equipment Excel Data' and wait for 'Progress: 100% Done'.<br>
            5. Click 'Download Equipment Excel Data' to download.<br><br>"),
      
      textAreaInput("maintenance_contact", "Contact Information", value = "xyang1@apotex.com, Tel:416-8888-888")
    )
  )
)

# -------------------------------------------------------------------------
# Server: Backend logic
# -------------------------------------------------------------------------
server <- function(input, output, session) {
  
  output$equipment_dropdown <- renderUI({
    site_name <- input$site
    equipment_list <- switch(site_name,
                             "LPDE_Weston_Stage2_Equipment"=LPDE_Weston_Stage2_Equipment,
                             "LPDE_Signet_Stage2_Equipment"=LPDE_Signet_Stage2_Equipment,
                             "LPDE_Garyray_Stage2_Equipment"=LPDE_Garyray_Stage2_Equipment,
                             "LPDE_Etobicoke_Stage2_Equipment"=LPDE_Etobicoke_Stage2_Equipment
    )
    selectInput("equipment", "Select Equipment", choices = equipment_list, multiple = FALSE)
  })
  
  Report_Writing_Identifier <- reactiveVal(NULL)
  
  observe({
    req(input$equipment)
    Report_Writing_Identifier(input$equipment)
  })
  
  observeEvent(input$generate_report, {
    equipment_names <- input$equipment
    Time_Period_Initial <- as.character(input$Time_Period_Initial)
    Time_Period_End <- as.character(input$Time_Period_End)
    original_working_directory <- paste0(install_folder, "\\CV_SARWAS_LPDE\\LowPDE_Templates")
    withProgress(
      session = session,
      message = 'Generating Reports...',
      detail = '',
      value = 0,
      {for (i in seq_along(equipment_names)) {
        Report_Writing_Identifier <- equipment_names[i]
        Sys.setenv(REPORT_WRITING_IDENTIFIER = Report_Writing_Identifier)
        Sys.setenv(TIME_PERIOD_INITIAL = Time_Period_Initial)
        Sys.setenv(TIME_PERIOD_END = Time_Period_End)
        Sys.setenv(INSTALL_FOLDER=install_folder )
        output_file <- paste0("0", equipment_names[i], ".pdf")
        setwd(original_working_directory)
        quarto::quarto_render(
          file.path(original_working_directory, "_cv08_2_Low_PDE_Automation_Stage2.qmd"),
          output_file = output_file
        )
        setwd(original_working_directory)
        new_output_file <- file.path(paste0(install_folder, "\\cv_sarwas_lpde\\Reports_and_Data_Stage2_LPDE\\", equipment_names[i], ".pdf"))
        file.rename(output_file, new_output_file)
        
        incProgress(1 / length(equipment_names), detail = paste('Generated report for', equipment_names[i]))
      }
        
        setProgress(1, message = 'Done!')
      } ) })
  
  output$download_report <- downloadHandler(
    filename = function() {
      paste("0", input$equipment, ".pdf", sep = "")
    },
    content = function(file) {
      pdf_files <- file.path(paste0(install_folder,"\\CV_SARWAS_LPDE\\Reports_and_Data_Stage2_LPDE\\", input$equipment, ".pdf"))
      pdftools::pdf_combine(pdf_files, output = file)
    }
  )
  
  output$progress_report <- renderText({
    if (!isTruthy(input$generate_report)) {
      return("Progress: 0% Done")
    }
    "Progress: 100% Done"
  })
  
  current_queries <- reactiveVal(NULL)
  queries <- reactive({
    equipment_name <- as.character(Report_Writing_Identifier())
    list(
      DAR = sprintf("SELECT * FROM infolabs_dev.cv_processed.LPDE_Stage2_signet_garyray_weston_etobicoke WHERE DataSetName='%s'", equipment_name)
      
    )
  })
  
  observeEvent(input$generate_data, {
    Time_Period_Initial <- as.Date(input$Time_Period_Initial, format = "%Y-%m-%d")
    Time_Period_End <- as.Date(input$Time_Period_End, format = "%Y-%m-%d")
    
    withProgress(
      session = session,
      message = 'Generating Data...',
      detail = '',
      value = 0,
      {
        Report_Writing_Identifier <- input$equipment[1]
        Sys.setenv(REPORT_WRITING_IDENTIFIER = Report_Writing_Identifier)
        current_queries_val <- queries()
        total_queries <- length(current_queries_val)
        wb <- openxlsx::createWorkbook()
        for (i in seq_along(current_queries_val)) {
          sheet_name <- names(current_queries_val)[i]
          openxlsx::addWorksheet(wb, sheet_name)
          data_frame <- tryCatch({
            dbGetQuery(conn1, current_queries_val[[i]])
          }, error = function(e) {
            return(data.frame())
          })
          if (!inherits(data_frame$Sampling_Date, "POSIXct")) {
            data_frame$Sampling_Date <- as.POSIXct(data_frame$Sampling_Date)
          }
          
          # Corrected logical error to include the entire end date
          filtered_data <- data_frame[data_frame$Sampling_Date >= Time_Period_Initial &
                                        data_frame$Sampling_Date < (Time_Period_End+1), ]
          
          openxlsx::writeData(wb, sheet = sheet_name, x = filtered_data)
          incProgress(1 / total_queries, detail = sprintf('%d/%d queries completed', i, total_queries))
        }
        xlsx_file <- file.path(paste0(install_folder, "\\CV_SARWAS_LPDE\\Reports_and_Data_Stage2_LPDE\\", input$equipment[1], ".xlsx"))
        #xlsx_file <- file.path("C:\\Users\\xyang1\\CV_SARWAS_LPDE\\Reports_and_Data_Stage2_LPDE", paste0(input$equipment[1], ".xlsx"))
        openxlsx::saveWorkbook(wb, xlsx_file, overwrite = TRUE)
        incProgress(1, detail = 'Done!')
      }
    )
  })
  
  output$download_data <- downloadHandler(
    filename = function() {
      paste0(input$equipment[1], ".xlsx")
    },
    content = function(file) {
      xlsx_file <- file.path(paste0(install_folder, "\\CV_SARWAS_LPDE\\Reports_and_Data_Stage2_LPDE\\", input$equipment[1], ".xlsx"))
      #xlsx_file <- file.path(paste0(install_folder, "\\CV_SARWAS_LPDE\\Reports_and_Data_Stage2_LPDE"), paste0(input$equipment[1], ".xlsx"))
      file.copy(xlsx_file, file)
    }
  )
  
  observe({
    current_queries_val <- current_queries()
    
    if (input$generate_data > 0 && !is.null(current_queries_val)) {
      withProgress(
        session = session,
        message = 'Generating Data for Tabs...',
        detail = '',
        value = 0,
        {
          for (i in seq_along(current_queries_val)) {
            incProgress(1/length(current_queries_val), detail = sprintf('%d/%d tabs completed', i, length(current_queries_val)))
            Sys.sleep(0.1)
          }
          incProgress(1, message = 'Data Generation Complete!')
        })}})
  
  output$progress_data <- renderText({
    if (!isTruthy(input$generate_data)) {
      return("Progress: 0% Done")
    }
    "Progress: 100% Done"
  })}
runApp(list(ui = ui, server = server), launch.browser = TRUE)
shiny::reactlogShow()















library(sparklyr)
library(dplyr)
library(DBI)
library(quarto)
library(shiny)

# Load devtools first, as it's the main tool for installing specific versions.
if (!requireNamespace("devtools", quietly = TRUE)) {
  install.packages("devtools")
}
library(devtools)

# Define the package list with required versions
packages <- list(
  AER = "1.2-13", boot = "1.3-31", chromote = "0.4.0", 
  DBI = "1.2.3", DiagrammeR = "1.0.11", DiagrammeRsvg = "0.1", 
  dplyr = "1.1.4", dunn.test = "1.3.6", easyanova = "1.1.0", 
  gt = "0.11.0", forcats = "1.0.0", formattable = "0.2.1", 
  qcc = "2.7", gridExtra = "2.3", kableExtra = "1.4.0", 
  knitr = "1.48", lubridate = "1.9.3", ggplot2 = "3.5.1",
  lmtest = "0.9-40", openxlsx = "4.2.7.1", odbc = "1.5.0", 
  lme4 = "1.1-35.5", quarto = "1.4.4", reactlog = "1.1.1", 
  rlang = "1.1.4", rmarkdown = "2.28", readxl = "1.3.1", 
  rsvg = "2.6.1", shiny = "1.9.1", stringr = "1.5.1", 
  tidyr = "1.3.1", tinytex = "0.53", VCA = "1.5.1",
  webshot2 = "0.1.1"
)

# Function to install a specific version if not already installed
install_if_needed <- function(package, version) {
  if (!requireNamespace(package, quietly = TRUE) || packageVersion(package) < version) {
    devtools::install_version(package, version = as.character(version), dependencies = TRUE)
  }
}

# -------------------------
# PASS 1: Install everything first (no loading)
# -------------------------
for (pkg in names(packages)) {
  install_if_needed(pkg, packages[[pkg]])
}

# Install and load custom CleaningValidation package
if (!requireNamespace("CleaningValidation", quietly = TRUE)) {
  system("R CMD INSTALL /dbfs/FileStore/CleaningValidation_1_0_tar.gz")
}
if (!requireNamespace("CleaningValidation", quietly = TRUE)) {
  stop("Error: The 'CleaningValidation' package could not be loaded after installation. Please check the path.")
}

# -------------------------
# PASS 2: Load all packages now
# -------------------------
for (pkg in names(packages)) {
  library(pkg, character.only = TRUE)
}
library(CleaningValidation)






library(sparklyr)
library(dplyr)
library(DBI)
library(quarto)
library(shiny)

# Load devtools first, as it's the main tool for this process.
if (!requireNamespace("devtools", quietly = TRUE)) {
  install.packages("devtools")
}
library(devtools)

# Your package list remains the same.
packages <- list(
  AER = "1.2-13", boot = "1.3-31", chromote = "0.4.0", 
  DBI = "1.2.3", DiagrammeR = "1.0.11", DiagrammeRsvg = "0.1", 
  dplyr = "1.1.4", dunn.test = "1.3.6", easyanova = "1.1.0", 
  gt = "0.11.0", forcats = "1.0.0", formattable = "0.2.1", 
  qcc = "2.7", gridExtra = "2.3", kableExtra = "1.4.0", 
  knitr = "1.48", lubridate = "1.9.3", ggplot2 = "3.5.1",
  lmtest = "0.9-40", openxlsx = "4.2.7.1", odbc = "1.5.0", 
  lme4 = "1.1-35.5", quarto = "1.4.4", reactlog = "1.1.1", 
  rlang = "1.1.4", rmarkdown = "2.28", readxl = "1.3.1", 
  rsvg = "2.6.1", shiny = "1.9.1", stringr = "1.5.1", 
  tidyr = "1.3.1", tinytex = "0.53", VCA = "1.5.1",
  webshot2 = "0.1.1"
)

# This is the key function. It checks if a package and its version
# are correct. If not, it installs the right version, including dependencies.
install_and_load <- function(package, version) {
  if (!requireNamespace(package, quietly = TRUE) || packageVersion(package) < version) {
    devtools::install_version(package, version = as.character(version))
  }
  library(package, character.only = TRUE)
}

# The main loop.
for (pkg in names(packages)) {
  install_and_load(pkg, packages[[pkg]])
}

# Install and load custom package after all dependencies are met.
if (!requireNamespace("CleaningValidation", quietly = TRUE)) {
  system("R CMD INSTALL /dbfs/FileStore/CleaningValidation_1_0_tar.gz")
}
if (!requireNamespace("CleaningValidation", quietly = TRUE)) {
  stop("Error: The 'CleaningValidation' package could not be loaded after installation. Please check the path.")
}
library(CleaningValidation)

Error in value[[3L]](cond) : 
  Package ‘car’ version 3.1.2 cannot be unloaded:
 Error in unloadNamespace(package) : namespace ‘car’ is imported by ‘AER’ so cannot be unloaded

Error in value[[3L]](cond): Package ‘car’ version 3.1.2 cannot be unloaded:
Error in value[[3L]](cond): Package ‘car’ version 3.1.2 cannot be unloaded:
 Error in unloadNamespace(package) : namespace ‘car’ is imported by ‘AER’ so cannot be unloaded









library(shiny)
library(sparklyr)
library(DBI)
library(quarto)

# Connect to Spark via Databricks once at startup
sc <- sparklyr::spark_connect(method = "databricks")

# Paths
qmd_path <- "/Workspace/Users/xyang1@apotex.com/CV_SARWAS_LPDE/CV_SARWAS_LPDE_Templates/_cv08_1_Databricks_Low_PDE_Automation_Stage3.qmd"
output_folder <- "/Workspace/Users/xyang1@apotex.com/CV_SARWAS_LPDE/Reports_and_Data_Stage3_LPDE/"
data_folder <- "infolabs_dev"

# Equipment lists
LPDE_Weston_Stage3_Equipment   <- dbGetQuery(sc, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Weston_Stage3_Equipment")[, 1]
LPDE_Signet_Stage3_Equipment   <- dbGetQuery(sc, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Signet_Stage3_Equipment")[, 1]
LPDE_Garyray_Stage3_Equipment  <- dbGetQuery(sc, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Garyray_Stage3_Equipment")[, 1]
LPDE_Etobicoke_Stage3_Equipment<- dbGetQuery(sc, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Etobicoke_Stage3_Equipment")[, 1]

LPDE_Signet_Etobicoke <- c(
  LPDE_Signet_Stage3_Equipment,
  LPDE_Weston_Stage3_Equipment,
  LPDE_Garyray_Stage3_Equipment,
  LPDE_Etobicoke_Stage3_Equipment
)

# UI
ui <- fluidPage(
  titlePanel("Input Time Period for Report Generation"),
  sidebarLayout(
    sidebarPanel(
      dateInput("Time_Period_Initial", "Start Date", value = Sys.Date() - 10000),
      dateInput("Time_Period_End", "End Date", value = Sys.Date()),
      actionButton("submit_time", "Submit Dates")
    ),
    mainPanel(
      verbatimTextOutput("time_message")
    )
  )
)

# Server
server <- function(input, output, session) {
  observeEvent(input$submit_time, {
    Time_Period_Initial <- as.character(input$Time_Period_Initial)
    Time_Period_End <- as.character(input$Time_Period_End)

    if (as.Date(Time_Period_Initial) > as.Date(Time_Period_End)) {
      output$time_message <- renderText("Error: Start date cannot be later than the end date.")
      return()
    }

    if (!file.exists(qmd_path)) {
      output$time_message <- renderText(paste("Error: Quarto input file not found at", qmd_path))
      return()
    }

    for (Report_Writing_Identifier in LPDE_Signet_Etobicoke) {
      params <- list(
        data_folder = data_folder,
        Report_Writing_Identifier = Report_Writing_Identifier,
        Time_Period_Initial = Time_Period_Initial,
        Time_Period_End = Time_Period_End
      )

      output_file_name <- paste0(Report_Writing_Identifier, ".pdf")
      default_output_file <- file.path(dirname(qmd_path), "_cv08_1_Databricks_Low_PDE_Automation_Stage3.pdf")
      output_full_path <- file.path(output_folder, output_file_name)

      tryCatch({
        quarto_render(
          input = qmd_path,
          output_format = "pdf",
          execute_params = params
        )
        if (file.exists(default_output_file)) {
          file.rename(default_output_file, output_full_path)
        } else {
          stop("The default output file was not created by Quarto.")
        }
      }, error = function(e) {
        output$time_message <- renderText(
          paste("Error generating report for", Report_Writing_Identifier, ":", e$message)
        )
        return()
      })
    }

    output$time_message <- renderText(paste("All reports have been generated and saved to", output_folder))
  })
}

shinyApp(ui = ui, server = server)




# Connect to Spark via Databricks
sc <- sparklyr::spark_connect(method = "databricks")
qmd_path <- "/Workspace/Users/xyang1@apotex.com/CV_SARWAS_LPDE/CV_SARWAS_LPDE_Templates/_cv08_1_Databricks_Low_PDE_Automation_Stage3.qmd"
output_folder <- "/Workspace/Users/xyang1@apotex.com/CV_SARWAS_LPDE/Reports_and_Data_Stage3_LPDE/"
data_folder <- "infolabs_dev"
LPDE_Weston_Stage3_Equipment <- DBI::dbGetQuery(sc, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Weston_Stage3_Equipment")[, 1]
LPDE_Signet_Stage3_Equipment <- DBI::dbGetQuery(sc, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Signet_Stage3_Equipment")[, 1]
LPDE_Garyray_Stage3_Equipment <- DBI::dbGetQuery(sc, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Garyray_Stage3_Equipment")[, 1]
LPDE_Etobicoke_Stage3_Equipment <- DBI::dbGetQuery(sc, "SELECT * FROM infolabs_dev.cv_processed.LPDE_Etobicoke_Stage3_Equipment")[, 1]
LPDE_Signet_Etobicoke <- c(LPDE_Signet_Stage3_Equipment, LPDE_Weston_Stage3_Equipment, LPDE_Garyray_Stage3_Equipment, LPDE_Etobicoke_Stage3_Equipment)
ui <- fluidPage(
  titlePanel("Input Time Period for Report Generation"),
  sidebarLayout(
    sidebarPanel(
      dateInput("Time_Period_Initial", "Start Date", value=Sys.Date()-10000),
      dateInput("Time_Period_End", "End Date", value=Sys.Date()),
      actionButton("submit_time", "Submit Dates")
    ),
    mainPanel(
      verbatimTextOutput("time_message")
    )
  )
)
server <- function(input, output, session) {
  observeEvent(input$submit_time, {
    Time_Period_Initial <- as.character(input$Time_Period_Initial)
    Time_Period_End <- as.character(input$Time_Period_End)
    if (as.Date(Time_Period_Initial) > as.Date(Time_Period_End)) {
      output$time_message <- renderText("Error: Start date cannot be later than the end date.")
      return()
    }
   output$time_message <- renderText(paste("Selected Time Period: ", Time_Period_Initial, " to ", Time_Period_End))
    if (!file.exists(qmd_path)) {
      output$time_message <- renderText(paste("Error: Quarto input file not found at", qmd_path))
      return()
    }
    for (Report_Writing_Identifier in LPDE_Signet_Etobicoke) {
      params <- list(
        data_folder = data_folder,
        Report_Writing_Identifier = Report_Writing_Identifier,
        Time_Period_Initial = Time_Period_Initial,
        Time_Period_End = Time_Period_End
      )
    output_file_name <- paste0(Report_Writing_Identifier, ".pdf")
    # Quarto will save the output file next to the input file by default.
    default_output_file <- file.path(dirname(qmd_path), "_cv08_1_Databricks_Low_PDE_Automation_Stage3.pdf")
    # Define the final path where the file should be moved
    output_full_path <- file.path(output_folder, output_file_name)
      
      tryCatch({
        # Render the Quarto document without specifying the output path,
        # which will save it to the default location.
        quarto::quarto_render(
          input = qmd_path,
          output_format = "pdf",
          execute_params = params
        )
      # After rendering, rename/move the file to the desired output folder and name.
        if (file.exists(default_output_file)) {
          file.rename(default_output_file, output_full_path)
        } else {
          stop("The default output file was not created by Quarto.")
        }
      }, error = function(e) {
        output$time_message <- renderText(paste("Error generating report for", Report_Writing_Identifier, ":", e$message))
        return()
      })
    }
  output$time_message <- renderText(paste("All reports have been generated and saved to", output_folder))
  })
}
shinyApp(ui = ui, server = server)



_cv08_1_Databricks_Low_PDE_Automation_Stage3.qmd file:

---
format:
  pdf:
    documentclass: article
    keep-tex: false
    number-sections: true
    toc: true
    toc-depth: 4
    latex-engine: xelatex
    geometry: [left=2cm, right=2cm, top=2cm, bottom=2cm]
header-includes:
  - \usepackage{caption}
  - \captionsetup[table]{labelformat=simple, labelsep=colon,textfont=bf}
  - \captionsetup[figure]{labelformat=simple, labelsep=colon, textfont=bf}
  - \usepackage{hyperref}
  - \usepackage{ragged2e}
  - \usepackage{fancyhdr}
  - \pagestyle{empty}
  - \fancyhf{}
  - \usepackage{colortbl}
  - \usepackage{tabu}
  - \usepackage{float}
  - \floatplacement{table}{H}
  - \floatplacement{figure}{H}
  - \floatplacement{figure}{!ht}
  - \usepackage{longtable,booktabs}
  - \pagenumbering{gobble}
fontsize: 12pt
bibliography: ["_references.bib"]
biblio-style: apalike
link-citations: true
linkcolor: brown
params:
  Report_Writing_Identifier: "abc"
  Time_Period_Initial: "2000-07-01"
  Time_Period_End: "2025-08-04"
  data_folder: "infolabs_dev"
---

\newpage
\justifying

```{r global_parameter_1}
#| echo: false
#| message: false
#| warning: false
knitr::opts_chunk$set(echo = FALSE, warning = FALSE,  message = FALSE)
```

```{r package_installation}
#| echo: false
#| message: false
#| warning: false
#| results: hide
options(repos = c(CRAN = "https://cloud.r-project.org/"))
if (!requireNamespace("devtools", quietly = TRUE)) {
  install.packages("devtools")
}
library(devtools)
packages <- list(
AER = "1.2-13",    boot = "1.3-31",       chromote = "0.4.0", 
DBI = "1.2.3",     DiagrammeR = "1.0.11", DiagrammeRsvg = "0.1", 
dplyr = "1.1.4",   dunn.test = "1.3.6",   easyanova = "1.1.0", 
gt = "0.11.0",     forcats = "1.0.0",     formattable = "0.2.1", 
qcc = "2.7",       gridExtra = "2.3",     kableExtra = "1.4.0", 
knitr = "1.48",    lubridate = "1.9.3",   ggplot2 = "3.5.1",
lmtest = "0.9-40", openxlsx = "4.2.7.1",  odbc = "1.5.0", 
lme4 = "1.1-35.5", quarto = "1.4.4",      reactlog="1.1.1", 
rlang = "1.1.4",   rmarkdown = "2.28",    readxl = "1.3.1", 
rsvg = "2.6.1",    shiny = "1.9.1",       stringr = "1.5.1", 
tidyr = "1.3.1",   tinytex = "0.53",      VCA = "1.5.1",
webshot2 = "0.1.1")

install_and_load <- function(package, version) {
  if (!requireNamespace(package, quietly = TRUE) || packageVersion(package) < version) {
    install_version(package, version = as.character(version))
  }
  library(package, character.only = TRUE)
}
for (pkg in names(packages)) { install_and_load(pkg, packages[[pkg]]) }

if (!requireNamespace("CleaningValidation", quietly = TRUE)) {
    system("R CMD INSTALL /dbfs/FileStore/CleaningValidation_1_0_tar.gz")
}
library(CleaningValidation)

tryCatch({
  tinytex::tlmgr_repo("https://mirror.ctan.org/systems/texlive/tlnet")
  
}, error = function(e) {
  tryCatch({
    tinytex::tlmgr_repo("https://mirror.csclub.uwaterloo.ca/CTAN/")
  }, error = function(e2) {
    warning("Both tinytex mirrors failed. Proceeding with caution.")
  })
})

tinytex::tlmgr_install(c("koma-script", "tabu", "environ", "colortbl", "pdflscape", "threeparttablex", "threeparttable", "makecell", "ragged2e", "lastpage", "multirow", "wrapfig", "ulem", "fancyhdr"))
```

```{r set_working_directory}
#| results: hide
library(DBI)
library(odbc)
library(SparkR)
sparkR.session()
Report_Writing_Identifier<-params$Report_Writing_Identifier
data_folder<-params$data_folder
#Time_Period_Initial <- as.Date(params$Time_Period_Initial)
#Time_Period_End <- as.Date(params$Time_Period_End)

 #conn1 <- DBI::dbConnect(
  #  drv = odbc::databricks(),
  #  httpPath = "/sql/1.0/warehouses/b2ef4b7a2c18fdcb",
  # workspace = "adb-4117502575300068.8.azuredatabricks.net",
  #  authMech = 11,
  #  auth_flow = 2
 # )
#Report_Writing_Identifier<-"cl1032_varen_cremer_tablet_filler_girton_4100weston"
#data_folder<-"infolabs_dev"
#params$data_folder

queries <- list(cv_dar= paste0("SELECT * FROM ", data_folder, ".cv_processed.lpde_signet_garyray_weston_etobicoke WHERE DataSetName='", Report_Writing_Identifier,"'"))
#queries <- list(cv_dar= paste0("SELECT * FROM infolabs_dev.cv_processed.lpde_signet_garyray_weston_etobicoke WHERE DataSetName='", Report_Writing_Identifier,"'"))
results_list <- list()
results_list <- lapply(queries, function(query) {
  tryCatch({
    result <- SparkR::collect(SparkR::sql(query))
     }, error = function(e) {
     data.frame() 
  })
 })

#results_list <- list()
#results_list <- lapply(queries, function(query) {
#  tryCatch({
#    dbGetQuery(sc, query)
#  }, error = function(e) {
#    data.frame()  
#  })
#})
```
```{r set_working_directoryx}
#| results: hide
# Time_Period_Initial <- "2000-07-01"
# Time_Period_End <- "2025-07-01"
Time_Period_Initial <- params$Time_Period_Initial
Time_Period_End <- params$Time_Period_End
Time_Period_Initial <- as.POSIXct(paste(Time_Period_Initial, "00:00:00"), tz = "UTC")
Time_Period_End <- as.POSIXct(paste(Time_Period_End, "23:59:59"), tz = "UTC")
#Time_Period_Initial <- as.POSIXct(Time_Period_Initial, format = "%m/%d/%Y", tz = "UTC")
#Time_Period_End <- as.POSIXct(Time_Period_End, format = "%m/%d/%Y", tz = "UTC")
Eq_DAR1 <- as.data.frame(results_list$cv_dar)
Eq_DAR1 <- Eq_DAR1[!tolower(gsub("[^a-zA-Z0-9]", "", Eq_DAR1$Swab_Reason)) %in% c("reswab"), ]
#Eq_DAR1 <- Eq_DAR1[Eq_DAR1$Sampling_Date >=Time_Period_Initial & Eq_DAR1$Sampling_Date <=Time_Period_End, ]
Eq_DAR <- Eq_DAR1[!tolower(Eq_DAR1$Classification) %in% c("proreoos", "nonprooos") & 
  tolower(Eq_DAR1$Swab_Reason) != "reswab", ]
Eq_DAR <- Eq_DAR[!is.na(Eq_DAR$DAR) & !is.na(Eq_DAR$USL) & !is.na(Eq_DAR$CleaningEvent), ]
Eq_CAR<-data.frame()
Eq_Mic<-data.frame()
Eq_CAR1<-data.frame()
Eq_Mic1<-data.frame()
```

```{r DAR_OOS_Reswab_table_4x}
OOS_Reswab_df <- CleaningValidation::cv02_1_nonpro_oos_reswab(Eq_DAR1, Eq_CAR1, Eq_Mic1)
```

```{r DataTransformation}
format_sop <- function(sop) {
  return(paste("{", paste(sop, collapse = ", "), "}", sep = ""))
}

if (nrow(Eq_DAR) > 0) {
  Eq_DAR <- CleaningValidation::cv01_dfclean(data=Eq_DAR, residue_col="DAR", cleaning_event_col="CleaningEvent", usl_col="USL")
  clean_process_sop <- format_sop(Eq_DAR$Cleaning_Process_SOP[nrow(Eq_DAR)])
  process_equipment <- Eq_DAR$Equipment[nrow(Eq_DAR)]
  process_equipment <- sub("\\([^()]*\\)$", "", process_equipment)  
  process_equipment <- gsub("&", "and", process_equipment) 
  process_equipment <- gsub("-", "", process_equipment)
  process_equipment <- gsub("[^a-zA-Z0-9,]", " ", process_equipment) 
  process_equipment <- gsub("\\s+", " ", process_equipment)
  process_equipment <- trimws(process_equipment)
  process_site_address <- Eq_DAR$Site_Address[1]
  process_Annual_Review_As_Per <- Eq_DAR$Annual_Review_As_Per[1]
} else {
    Eq_DAR <- data.frame()
}

fiscal_year <- ifelse(format(Time_Period_End, "%m") %in% c("01", "02", "03"),
                      substr(format(Time_Period_End, "%Y"), 3, 4),
                      substr(format(as.POSIXct(Time_Period_End) + 365*24*60*60, "%Y"), 3, 4))
process_equipment1 <- sub("^[^_]+_[^_]+_[^_]+_", "", params$Report_Writing_Identifier)
#process_equipment1 <- sub(".*cl[0-9]+_", "", params$Report_Writing_Identifier)  # Remove the initial "cl012" and its underscore
process_equipment1 <- sub("_[^_]*$", "", process_equipment1)  # Remove the last underscore and characters following it
process_equipment1 <- gsub("_", " ", process_equipment1)  # Replace remaining underscores with spaces
# 
# process_equipment <- sub("^[^_]+_[^_]+_", "", params$Report_Writing_Identifier)
# #process_equipment <- sub("_[^_]*$", "", params$Report_Writing_Identifier)
# process_equipment <- gsub("_", " ", process_equipment)
# process_equipment <- toupper(process_equipment)

if (nrow(Eq_DAR) > 0) {
dynamic_title <- toupper(paste0("CV FY", fiscal_year, " Low PDE Annual Statistical Report for ", Eq_DAR$Molecule[1], " on ", process_equipment,   " at ", Eq_DAR$Site_Address[nrow(Eq_DAR)]))
} else {
dynamic_title <- "EMPTY DATA HAS NO REPORT" 
}
Eq_DAR_Val <- Eq_DAR[grepl("val_pde|ver_pde", tolower(Eq_DAR$Swab_Reason)), ]
Eq_DAR_Mon <- Eq_DAR[grepl("mon_pde", tolower(Eq_DAR$Swab_Reason)), ]
```
---
title: "`r dynamic_title`"
---

```{r OverallDataCondition_1}
NumEventEq_DAR <- length(unique(Eq_DAR$CleaningEvent))
NumEventEq_DAR_Val <- length(unique(Eq_DAR_Val$CleaningEvent))
NumEventEq_DAR_Mon <- length(unique(Eq_DAR_Mon$CleaningEvent))
```

```{r DAR_condition}
if(NumEventEq_DAR>1){
Eq_DAR <- CleaningValidation::cv03_usl_unification(data=Eq_DAR,cleaning_event_col="CleaningEvent",residue_col="DAR",usl_col = "USL")
summary_sentence_DAR_outlier <- cv_oos_outlier_comm(Eq_DAR, "DAR_Pct", "CleaningEvent", "USL_Pct")
result_C_DAR<-cv07_1_median_control_chart(data=Eq_DAR,cleaning_event_col="CleaningEvent",residue_pct_median_col = "DAR_Pct_Median")
Child_DAR=(summary_sentence_DAR_outlier$num_outliers>0)|(result_C_DAR$num_OOC>0)
D0010 <- (length(unique(Eq_DAR$DAR))>1)&(Child_DAR==F)
D0011 <- (length(unique(Eq_DAR$DAR))>1)&(Child_DAR==T)
} else {
  D0010 <-F
  D0011 <- F
}
```

```{r Condition_classification}
D100 <- (NumEventEq_DAR<10)
D010 <- (length(unique(Eq_DAR$DAR))==1)&(!D100)
D0010 <- D0010&(!D100)
D0011 <- D0011&(!D100)
ST <- (NumEventEq_DAR_Mon==0)
```

```{r ,  child=if(ST & D100){'_cv01_STD100.qmd'}}
```

```{r ,  child=if(ST & D010){'_cv05_STD010.qmd'}}
```

```{r ,  child=if(ST & D0010){'_cv06_STD0010.qmd'}}
```

```{r ,  child=if(ST & D0011){'_cv07_STD0011.qmd'}}
```

```{r ,  child=if(!ST & D010){'_cv02_D010.qmd'}}
```

```{r ,  child=if(!ST & D0010){'_cv03_D0010.qmd'}}
```

```{r ,  child=if(!ST & D0011){'_cv04_D0011.qmd'}}
```




















---
title: "SparkR::sql, DBI::dbExecute, and sparklyr::spark_sql"
format: pdf
author: Xiande Yang
date: "2025-08-02"
editor: visual
---
library(sparklyr)
library(dplyr)
library(glue)
library(tibble)

# Connect to Databricks
sc <- spark_connect(method = "databricks")

# Load Spark table
alpdesetall_signet_spark <- tbl(sc, "infolabs_dev.cv_processed.alpdesetall_signet")

# Get equipment with 'mon' in Swab_Reason
equipment_with_mon_reason_df <- alpdesetall_signet_spark %>%
  filter(grepl("mon", tolower(Swab_Reason))) %>%
  distinct(DataSetName) %>%
  collect()

equipment_with_mon_reason_vector <- equipment_with_mon_reason_df$DataSetName

# Write Stage 3 table
LPDE_Signet_Stage3 <- alpdesetall_signet_spark %>%
  filter(DataSetName %in% equipment_with_mon_reason_vector)

spark_write_table(
  LPDE_Signet_Stage3,
  name = "infolabs_dev.cv_processed.LPDE_Signet_Stage3",
  mode = "overwrite",
  temporary = FALSE
)

# Write Stage 3 equipment list
LPDE_Signet_Stage3_Equipment <- tibble(equipment = equipment_with_mon_reason_vector)

LPDE_Signet_Stage3_Equipment_spark <- copy_to(sc, LPDE_Signet_Stage3_Equipment, "LPDE_Signet_Stage3_Equipment_temp", overwrite = TRUE)

spark_write_table(
  LPDE_Signet_Stage3_Equipment_spark,
  name = "infolabs_dev.cv_processed.LPDE_Signet_Stage3_Equipment",
  mode = "overwrite",
  temporary = FALSE
)

# Use pure SQL to avoid mutate/sql() issue in Stage 2
excluded_equipment_sql <- paste0("'", paste(equipment_with_mon_reason_vector, collapse = "','"), "'")

query <- glue("
SELECT 
  DataSetName
FROM (
  SELECT 
    DataSetName,
    MAX(CAST(regexp_extract(CleaningEvent, '\\\\d+', 0) AS INT)) AS max_event
  FROM infolabs_dev.cv_processed.alpdesetall_signet
  WHERE DataSetName NOT IN ({excluded_equipment_sql})
  GROUP BY DataSetName
)
WHERE max_event >= 10
")

equipment_for_stage2_df <- DBI::dbGetQuery(sc, query)
equipment_for_stage2_vector <- equipment_for_stage2_df$DataSetName

# Write Stage 2 table
LPDE_Signet_Stage2 <- alpdesetall_signet_spark %>%
  filter(DataSetName %in% equipment_for_stage2_vector)

spark_write_table(
  LPDE_Signet_Stage2,
  name = "infolabs_dev.cv_processed.LPDE_Signet_Stage2",
  mode = "overwrite",
  temporary = FALSE
)

# Write Stage 2 equipment list
LPDE_Signet_Stage2_Equipment <- tibble(equipment = equipment_for_stage2_vector)

LPDE_Signet_Stage2_Equipment_spark <- copy_to(sc, LPDE_Signet_Stage2_Equipment, "LPDE_Signet_Stage2_Equipment_temp", overwrite = TRUE)

spark_write_table(
  LPDE_Signet_Stage2_Equipment_spark,
  name = "infolabs_dev.cv_processed.LPDE_Signet_Stage2_Equipment",
  mode = "overwrite",
  temporary = FALSE
)

message("✅ All tables written without mutate/sql warnings.")



library(sparklyr)
library(dplyr)
library(tibble) # For creating local R tibbles if needed
sc <- spark_connect(method = "databricks")
alpdesetall_signet_spark <- tbl(sc, "infolabs_dev.cv_processed.alpdesetall_signet")
equipment_with_mon_reason_df <- alpdesetall_signet_spark %>%
 filter(grepl("mon", tolower(Swab_Reason))) %>% # Removed fixed = TRUE
 distinct(DataSetName) %>%
 collect()

equipment_with_mon_reason_vector <- equipment_with_mon_reason_df$DataSetName
LPDE_Signet_Stage3_Equipment <- tibble(equipment = equipment_with_mon_reason_vector)
LPDE_Signet_Stage3 <- alpdesetall_signet_spark %>%
 filter(DataSetName %in% equipment_with_mon_reason_vector)

spark_write_table(LPDE_Signet_Stage3,
name = "infolabs_dev.cv_processed.LPDE_Signet_Stage3",
mode = "overwrite", # Use "append" to add to existing table
 temporary = FALSE) # Set to TRUE if you only need it for the current session


LPDE_Signet_Stage3_Equipment_spark <- copy_to(sc, LPDE_Signet_Stage3_Equipment, "LPDE_Signet_Stage3_Equipment_temp", overwrite = TRUE)

spark_write_table(LPDE_Signet_Stage3_Equipment_spark,
name = "infolabs_dev.cv_processed.LPDE_Signet_Stage3_Equipment",
 mode = "overwrite",
temporary = FALSE)

equipment_to_exclude_from_stage2 <- equipment_with_mon_reason_vector

equipment_for_stage2_df <- alpdesetall_signet_spark %>%
 filter(!(DataSetName %in% equipment_to_exclude_from_stage2)) %>% # Exclude entire DataSetNames if any record has "mon"
mutate(cleaning_event_num = as.numeric(regexp_extract(CleaningEvent, "\\\\d+", 0))) %>% # Extracts numbers from CleaningEvent
 group_by(DataSetName) %>%
 summarise(max_cleaning_event = max(cleaning_event_num, na.rm = TRUE)) %>% # Find max cleaning event number per equipment
 filter(max_cleaning_event >= 10) %>%
 distinct(DataSetName) %>%
 collect() # Bring distinct DataSetNames to R as a local data frame


# Convert the distinct DataSetName column to a character vector for easier filtering

equipment_for_stage2_vector <- equipment_for_stage2_df$DataSetName
LPDE_Signet_Stage2_Equipment <- tibble(equipment = equipment_for_stage2_vector)
message("LPDE_Signet_Stage2_Equipment (local R tibble) created.")


LPDE_Signet_Stage2 <- alpdesetall_signet_spark %>%
filter(DataSetName %in% equipment_for_stage2_vector)
spark_write_table(LPDE_Signet_Stage2,
 name = "infolabs_dev.cv_processed.LPDE_Signet_Stage2",
 mode = "overwrite",
 temporary = FALSE)

LPDE_Signet_Stage2_Equipment_spark <- copy_to(sc, LPDE_Signet_Stage2_Equipment, "LPDE_Signet_Stage2_Equipment_temp", overwrite = TRUE)
spark_write_table(LPDE_Signet_Stage2_Equipment_spark,
name = "infolabs_dev.cv_processed.LPDE_Signet_Stage2_Equipment",
mode = "overwrite",
 temporary = FALSE)
# SparkR, DBI, and sparklyr

Databricks deprecated SparkR using which I wrote more than 150k lines of code. This really made me upset. However, I had to change to sparklyr.

## sparkR::sql, sparklyr::spark_sql, dplyr::tbl(sc, dplyr::sql()), and DBI::dbExecute.

- sparkR::sql() can be used for parallel tasks which is essential for big data.

-sparklyr::sdf_sql() does not work for DDL commands.

- dplyr::tbl(sc, dplyr::sql()) is good for select operation in SQL or dplyr equivalent operations excellent for pipe operation %>% but it does not work for create or replace table etc.

- DBI::dbExecute() works equivalent to sparkR:sql() or sparklyr::spark_sql() however, it is for sequential tasks but not for parallel work. So, when I ran code parallelly in Databricks, it has the error of race condition.

I’m working on Databricks, which has deprecated the sparkR package and recommends using sparklyr. While transitioning, I’ve encountered a major limitation:

sparklyr::sdf_sql() and dplyr::sql() are designed for SELECT queries only — not for DDL commands like CREATE OR REPLACE TABLE.

DBI::dbExecute(sc, ...) can handle DDL, but it is not safe in parallel workflows. When I launch three parallel tasks (e.g., cv11, cv12, and cv13), each executing a CREATE OR REPLACE TABLE command, Databricks throws path overlap errors — classic race conditions.

sparkR::sql() works perfectly here — it’s thread-safe, and handles DDL well in parallel. But since SparkR is deprecated, I want to find a better long-term alternative.

I've tried:

sparklyr::invoke() to access Spark’s JVM backend. It works for DDL, but comes with side effects — for example, built-in functions like REGEXP_SUBSTR() fail due to an incorrect session context, breaking compatibility with other Spark SQL features.

Other options:

Writing %sql cells in Databricks Notebooks is safe and handles DDL — but this doesn’t integrate well into R scripts or R-based pipelines.

My question:
👉 Is there any reliable method or function (in sparklyr, dplyr, or elsewhere in the R ecosystem) that supports running DDL like CREATE OR REPLACE TABLE safely in parallel, within an R context on Databricks?

This gap is frustrating because parallelism is core to big data, and the lack of parallel-safe DDL support in sparklyr limits its usefulness in real production pipelines. At present, I still rely on SparkR::sql() because I have over 150k lines of R code and parallel execution is essential.

It would be great if sparklyr could introduce a function like spark_sql() or support parallel-safe DDL operations directly.

Thanks in advance for any insight, workarounds, or best practices!









