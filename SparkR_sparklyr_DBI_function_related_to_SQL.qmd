---
title: "SparkR::sql, DBI::dbExecute, and sparklyr::spark_sql"
format: pdf
author: Xiande Yang
date: "2025-08-02"
editor: visual
---
library(sparklyr)
library(dplyr)
library(tibble) # For creating local R tibbles if needed
sc <- spark_connect(method = "databricks")
alpdesetall_signet_spark <- tbl(sc, "infolabs_dev.cv_processed.alpdesetall_signet")
equipment_with_mon_reason_df <- alpdesetall_signet_spark %>%
 filter(grepl("mon", tolower(Swab_Reason))) %>% # Removed fixed = TRUE
 distinct(DataSetName) %>%
 collect()

equipment_with_mon_reason_vector <- equipment_with_mon_reason_df$DataSetName
LPDE_Signet_Stage3_Equipment <- tibble(equipment = equipment_with_mon_reason_vector)
LPDE_Signet_Stage3 <- alpdesetall_signet_spark %>%
 filter(DataSetName %in% equipment_with_mon_reason_vector)

spark_write_table(LPDE_Signet_Stage3,
name = "infolabs_dev.cv_processed.LPDE_Signet_Stage3",
mode = "overwrite", # Use "append" to add to existing table
 temporary = FALSE) # Set to TRUE if you only need it for the current session


LPDE_Signet_Stage3_Equipment_spark <- copy_to(sc, LPDE_Signet_Stage3_Equipment, "LPDE_Signet_Stage3_Equipment_temp", overwrite = TRUE)

spark_write_table(LPDE_Signet_Stage3_Equipment_spark,
name = "infolabs_dev.cv_processed.LPDE_Signet_Stage3_Equipment",
 mode = "overwrite",
temporary = FALSE)

equipment_to_exclude_from_stage2 <- equipment_with_mon_reason_vector

equipment_for_stage2_df <- alpdesetall_signet_spark %>%
 filter(!(DataSetName %in% equipment_to_exclude_from_stage2)) %>% # Exclude entire DataSetNames if any record has "mon"
mutate(cleaning_event_num = as.numeric(regexp_extract(CleaningEvent, "\\\\d+", 0))) %>% # Extracts numbers from CleaningEvent
 group_by(DataSetName) %>%
 summarise(max_cleaning_event = max(cleaning_event_num, na.rm = TRUE)) %>% # Find max cleaning event number per equipment
 filter(max_cleaning_event >= 10) %>%
 distinct(DataSetName) %>%
 collect() # Bring distinct DataSetNames to R as a local data frame


# Convert the distinct DataSetName column to a character vector for easier filtering

equipment_for_stage2_vector <- equipment_for_stage2_df$DataSetName
LPDE_Signet_Stage2_Equipment <- tibble(equipment = equipment_for_stage2_vector)
message("LPDE_Signet_Stage2_Equipment (local R tibble) created.")


LPDE_Signet_Stage2 <- alpdesetall_signet_spark %>%
filter(DataSetName %in% equipment_for_stage2_vector)
spark_write_table(LPDE_Signet_Stage2,
 name = "infolabs_dev.cv_processed.LPDE_Signet_Stage2",
 mode = "overwrite",
 temporary = FALSE)

LPDE_Signet_Stage2_Equipment_spark <- copy_to(sc, LPDE_Signet_Stage2_Equipment, "LPDE_Signet_Stage2_Equipment_temp", overwrite = TRUE)
spark_write_table(LPDE_Signet_Stage2_Equipment_spark,
name = "infolabs_dev.cv_processed.LPDE_Signet_Stage2_Equipment",
mode = "overwrite",
 temporary = FALSE)
# SparkR, DBI, and sparklyr

Databricks deprecated SparkR using which I wrote more than 150k lines of code. This really made me upset. However, I had to change to sparklyr.

## sparkR::sql, sparklyr::spark_sql, dplyr::tbl(sc, dplyr::sql()), and DBI::dbExecute.

- sparkR::sql() can be used for parallel tasks which is essential for big data.

-sparklyr::sdf_sql() does not work for DDL commands.

- dplyr::tbl(sc, dplyr::sql()) is good for select operation in SQL or dplyr equivalent operations excellent for pipe operation %>% but it does not work for create or replace table etc.

- DBI::dbExecute() works equivalent to sparkR:sql() or sparklyr::spark_sql() however, it is for sequential tasks but not for parallel work. So, when I ran code parallelly in Databricks, it has the error of race condition.

Iâ€™m working on Databricks, which has deprecated the sparkR package and recommends using sparklyr. While transitioning, Iâ€™ve encountered a major limitation:

sparklyr::sdf_sql() and dplyr::sql() are designed for SELECT queries only â€” not for DDL commands like CREATE OR REPLACE TABLE.

DBI::dbExecute(sc, ...) can handle DDL, but it is not safe in parallel workflows. When I launch three parallel tasks (e.g., cv11, cv12, and cv13), each executing a CREATE OR REPLACE TABLE command, Databricks throws path overlap errors â€” classic race conditions.

sparkR::sql() works perfectly here â€” itâ€™s thread-safe, and handles DDL well in parallel. But since SparkR is deprecated, I want to find a better long-term alternative.

I've tried:

sparklyr::invoke() to access Sparkâ€™s JVM backend. It works for DDL, but comes with side effects â€” for example, built-in functions like REGEXP_SUBSTR() fail due to an incorrect session context, breaking compatibility with other Spark SQL features.

Other options:

Writing %sql cells in Databricks Notebooks is safe and handles DDL â€” but this doesnâ€™t integrate well into R scripts or R-based pipelines.

My question:
ðŸ‘‰ Is there any reliable method or function (in sparklyr, dplyr, or elsewhere in the R ecosystem) that supports running DDL like CREATE OR REPLACE TABLE safely in parallel, within an R context on Databricks?

This gap is frustrating because parallelism is core to big data, and the lack of parallel-safe DDL support in sparklyr limits its usefulness in real production pipelines. At present, I still rely on SparkR::sql() because I have over 150k lines of R code and parallel execution is essential.

It would be great if sparklyr could introduce a function like spark_sql() or support parallel-safe DDL operations directly.

Thanks in advance for any insight, workarounds, or best practices!



